{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txa5iGJPQqrf"
   },
   "source": [
    "# Lang Chain & Lang Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVoutpoOIyZG"
   },
   "source": [
    "Ce notebook est à la fois l'énoncé du TP et un moyen d'éxécuter directement votre code sur l'environnement proposé par Google Colab.\n",
    "\n",
    "- Commencez par créer un dossier dédié au TP sur votre propre drive (par exemple, `SFFS/TP_LangGraph`) puis copiez-y ce notebook (Fichier > Enregistrer une copie dans drive). **- Option recommandée, nécessite un compte Google**\n",
    "\n",
    "- Si vous préférez, vous pouvez plutôt travailler en local (téléchargez alors simplement le dossier `TP_LangGraph` complet) **- Charge à vous de gérer les éventuelles différences entre votre environnement et celui pour lequel ce TP est conçu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceNJq-LfVQz7"
   },
   "source": [
    "\n",
    "\n",
    "Ce TP vise à vous familiariser avec les concepts et l'implémentation d'applications basées sur des LLM et des agents IA. Nous explorerons l'utilisation de Python, de la bibliothèque LangChain pour l'intégration de LLM et d'outils, et de LangGraph pour l'orchestration de workflows multi-agents sous forme de graphes.\n",
    "\n",
    "Voici le déroulé du tp :\n",
    "Nous découvrirons tout d'abord comment créer des outils simples (comme une calculatrice) et les intégrer dans des chaînes pour permettre au LLM d'effectuer des tâches spécifiques.\n",
    "Puis nous mettrons en place des agents IA, et nous introduirons également LangSmith pour le traçage et le débogage de nos applications.\n",
    "Et pour finir, nous construirons une application multi-agents avec LangGraph pour assister les techniciens d'usine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFQZGt3fQuu9"
   },
   "source": [
    "## 1) Avant de commencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kVetFbCF5gf"
   },
   "source": [
    "**(N'exécutez les 2 prochaines cellules que si, comme recommandé, vous utilisez ce notebook dans Google Colab**). Commençons par monter le dossier dans Colab et copier les dépendances nécessaires au TP, dans un dossier `resources`à côté de ce notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpXBPWDNGRUs"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# # Si nécessaire, remplacez par l'emplacement du dossier contenant ce notebook au sein de votre drive\n",
    "# path = 'SFFS/TP_LangGraph'\n",
    "\n",
    "# # Monter le dossier dans Google Colab\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir(f'/content/drive/MyDrive/{path}')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFQD9Z814XCN"
   },
   "outputs": [],
   "source": [
    "# Copier le contenu du dossier `resources` à côté du notebook, dans votre drive (comme Google Drive ne permet pas de copier des dossiers directement)\n",
    "# A ne faire que si vous exécutez ce notebook POUR LA 1ERE FOIS, ET AVEC GOOGLE COLAB\n",
    "# !gdown --folder \"https://drive.google.com/drive/folders/1atbNPJCxeU5o2Mdlzf1ioKRJi5qc2WCR?usp=sharing\" -O ./resources\n",
    "\n",
    "# print(os.listdir()) # devrait afficher ['resources', 'LangGraph.ipynb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7fVdzoteRs1"
   },
   "source": [
    "Pour afficher les résultats correctement par la suite, ajustons la configuration du notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ra5O9ePequj"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"none\" # désactiver l'affichage de la variable à la dernière ligne de chaque cellule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viTWTmCTQ1BQ"
   },
   "source": [
    "Il faut ensuite installer les dépendences Python nécessaires et configurer notre LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMIoMeQ8RZbm"
   },
   "source": [
    "Commençons par **installer LangChain :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M22ktdXKQyLv"
   },
   "outputs": [],
   "source": [
    "# Installer LangChain (ignorez des erreurs de type \"pip's dependency resolver does not currently take into account all the packages that are installed\")\n",
    "%pip install --quiet langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_k2jFOZRdn9"
   },
   "source": [
    "Ensuite, LangChain nécessite qu'on lui fournisse un [endpoint](https://docs.langchain.com/oss/python/integrations/providers/overview#popular-providers) où il peut accéder à un LLM. Cet endpoint peut correspondre :\n",
    "- soit à une API, comme celle exposée par OpenAI ou Google,\n",
    "- soit à un LLM _opensource_ que l'on fait tourner en local.\n",
    "\n",
    "\n",
    "Par facilité, nous allons utiliser l'**API de Groq** (à ne pas confondre avec Grok, l'IA d'Elon Musk), qui propose un plan gratuit d'utilisation de plusieurs modèles _open source_ (comme llama), avec quelques restrictions sur la version du modèle, les requêtes par minute (RPM), les tokens par minute (TPM) et les requêtes par jour (RPD). Vous pouvez retrouver les restrictions ici : https://console.groq.com/docs/rate-limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L26-D3xOfGyA"
   },
   "source": [
    "Pour commencer, rendez-vous sur le site pour créer une clé : https://console.groq.com/keys. N'oubliez pas de la stocker quelque part.\n",
    "\n",
    "Ensuite, il suffit de la renseigner ici, en exécutant cette cellule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF3RnIGafpHn"
   },
   "outputs": [],
   "source": [
    "GROQ_API_KEY = input()\n",
    "\n",
    "# Nettoyer la sortie de la cellule par précaution\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f\"Clé enregistrée : {GROQ_API_KEY[:2]}...{GROQ_API_KEY[-2:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BbTEGTyK4t-"
   },
   "source": [
    "Pour ce TP, nous vous proposons d'utiliser `qwen/qwen3-32b` (https://huggingface.co/Qwen/Qwen3-32B). Ce modèle :\n",
    "- peut prendre en entrée un texte allant jusqu'à 131 072 tokens, et produire une sortie allant jusqu'à 40 960 tokens,\n",
    "- est **multi-langues** (vous pouvez donc lui parler en français, anglais, chinois, C++...),\n",
    "- supporte le raisonnement,\n",
    "- a des limites d'utilisations assez généreuses, sur Groq.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RumSmM0uTbkr"
   },
   "outputs": [],
   "source": [
    "model_name = \"qwen/qwen3-32b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shHYf2KqTuw8"
   },
   "source": [
    "Installons la librairie de LangChain requise pour **accéder à l'API de Groq :**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Pu3feNSQkB6"
   },
   "outputs": [],
   "source": [
    "# Nécessaire pour brancher un LLM exposé par l'API de Groq à LangChain\n",
    "%pip install --quiet langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_7L7qx4WnJK"
   },
   "source": [
    "**Instancions notre LLM** et **vérifions qu'il fonctionne correctement :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYr2HGQ1V8FZ"
   },
   "outputs": [],
   "source": [
    "# Créer un LLM connecté à l'API\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=model_name,\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    reasoning_effort=\"none\", # désactiver le raisonnement\n",
    "    model_kwargs={\"seed\": 42, \"top_p\": 1e-30} # pour garantir la reproductibilité\n",
    ")\n",
    "\n",
    "# Une petite fonction qui passe une entrée au LLM et renvoie la sortie\n",
    "from IPython.display import display, Markdown\n",
    "def invoke(llm: ChatGroq, input: str, display_output: bool = False):\n",
    "  output = llm.invoke(input).content\n",
    "\n",
    "  # Pour se faire plaisir, afficher la sortie en markdown (format souvent privilégié par ce LLM)\n",
    "  if display_output:\n",
    "    display(Markdown(output))\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kofCVYnX3-u"
   },
   "outputs": [],
   "source": [
    "# Poser une question au LLM pour vérifier que tout fonctionne\n",
    "test_input = \"Quelles sont les capitales de l'Australie, du Canada, du Brésil et de l'Afrique du Sud ?\"\n",
    "invoke(llm, test_input, display_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj4DulELyy2i"
   },
   "source": [
    "Pour finir, installons LangGraph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD4ABU50yzCK"
   },
   "outputs": [],
   "source": [
    "# Installer langGraph\n",
    "%pip install --quiet langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rMkPPVhbM9j"
   },
   "source": [
    "Maintenant que tout est prêt, nous allons construire pas-à-pas notre première application basée sur des agents !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TVBYjj2b8Eg"
   },
   "source": [
    "## 2) Nos premiers outils et chaînes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ8JhDOLg_c_"
   },
   "source": [
    "Documentation en cas de pépin : https://python.langchain.com/docs/how_to/tools_prompting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sTDdcz0bZfB"
   },
   "source": [
    "Comme vous avez écouté en cours, vous savez que LangChain permet de construire des applications basées sur des **agents**, c'est-à-dire des LLM qui ont accès à certains **outils (tools)** qu'ils peuvent utiliser pour résoudre des problèmes.\n",
    "\n",
    "Les outils sont des fonctions (en l'occurence, Python) auxquelles a accès l’agent, et qui peuvent par exemple effectuer des calculs, consulter une base de données, faire des appels à des APIs... Un outil ne peut pas déléguer le travail à un autre outil ou agent ; il s’occupe uniquement de retourner un résultat à l’agent qui l'appelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqcmgRCQfM48"
   },
   "source": [
    "Pour ce premier exemple, nous allons fournir une **calculatrice** à notre LLM.\n",
    "\n",
    "En effet, sans outils de calculs, certaines tâches peuvent être compliquées pour un LLM, dont l'architecture n'est pas conçue pour manipuler efficacement des nombres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olvaR7K8aGOr"
   },
   "outputs": [],
   "source": [
    "# Un petit problème de maths (le LLM va essayer de le résoudre SANS puis AVEC une calculatrice à sa disposition)\n",
    "puzzle1 = \"Si le nombre de bactéries augmente de 95 % chaque heure, par quel facteur sera multiplié la population initiale après 10h ?\"\n",
    "\n",
    "print(f\"Réponse attendue : {1.95**10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "680e-O6zfrjO"
   },
   "outputs": [],
   "source": [
    "invoke(llm, puzzle1, display_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AWNQfF7geab"
   },
   "source": [
    "Le LLM semble utiliser la bonne formule, mais se trompe complètement sur l'application numérique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUvEWowGgtfR"
   },
   "source": [
    "Maintenant, essayons **avec quelques outils de calcul** : `add`, `multiply` et `exponentiate` qui permettent de calculer respectivement une addition, un produit et une puissance (nous aurions pu ajouter d'autres opérations comme la division, le modulo...) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVYjzdq0f7ot"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(number1: float, number2: float) -> float:\n",
    "   \"\"\"Computes number1+number2, i.e. add the first argument to the second argument.\"\"\"\n",
    "   return number1 + number2\n",
    "\n",
    "@tool\n",
    "def multiply(number1: float, number2: float) -> float:\n",
    "   \"\"\"Computes number1*number2, i.e. multiply the first argument by the second argument.\"\"\"\n",
    "   return number1 * number2\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: float, power: float) -> float:\n",
    "   \"\"\"Computes base^power, i.e. raises the first argument to the power of the second argument.\"\"\"\n",
    "   return base ** power\n",
    "\n",
    "tools = [add, multiply, exponentiate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Td7lTNwhYkU"
   },
   "source": [
    "**Important :** Notez\n",
    "- le décorateur `@tool` au dessus de chaque fonction,\n",
    "- le commentaire qui décrit chaque fonction,\n",
    "- les annotations de types des arguments et des valeurs retournées,\n",
    "\n",
    "Ces indications sont utilisées par LangChain pour générer une description textuelle des outils, qui sera fournie au LLM pour qu'il puisse décider de quand/comment intéragir avec eux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78N1WYAih7Ob"
   },
   "outputs": [],
   "source": [
    "# Générer une description textuelle des outils\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bALyc8MviE0v"
   },
   "outputs": [],
   "source": [
    "# Créer un prompt système incluant le contexte, la description des outils et des règles de formatage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"\\\n",
    "You are an assistant that has access to the following set of tools.\n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use.\n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt.format(rendered_tools=rendered_tools)),\n",
    "     (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb_tJZhsk91o"
   },
   "outputs": [],
   "source": [
    "# Définissons une pipeline LangChain (aussi appelée \"chaîne\") : créer le prompt (prompt système + input de l'utilisateur) -> le fournir au LLM\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP9obXB2kH3z"
   },
   "source": [
    "À ce stade là, le LLM devrait maintenant :\n",
    "- avoir accès à la description des outils `add`, `multiply` et `exponentiate`\n",
    "- comprendre qu'il faut appeler `exponentiate` au moment de l'application numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-0lbYkpkgJ1"
   },
   "outputs": [],
   "source": [
    "# Invoquons la pipeline sur le même problème\n",
    "invoke(chain, puzzle1, display_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f86rT-BlpPb"
   },
   "source": [
    "=> Au lieu de retourner du texte, notre chaîne retourne maintenant un objet JSON qui appelle l'un des outils que nous avons défini, dans un format standard, avec les bons paramètres !\n",
    "\n",
    "=> Cet appel à l'outil n'a pas été traité, car la chaîne actuelle ne fait que produire cet appel, à partir du prompt système et du problème de maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zklObxmNmojy"
   },
   "source": [
    "Il ne reste plus qu'à ajouter 2 étapes à la pipeline :\n",
    "- Parser le JSON produit par le LLM\n",
    "- \"Router\" l'appel produit vers le bon outil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-u8j12-kflE"
   },
   "outputs": [],
   "source": [
    "# Etape de parsing JSON\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "674Goh3HpBYw"
   },
   "outputs": [],
   "source": [
    "# Etape de routage vers le bon outil\n",
    "from typing import Any, Dict, Optional, TypedDict\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"Un appel à un outil, dans le format produit par le LLM\"\"\"\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "def invoke_tool(tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None):\n",
    "    \"\"\"Une fonction qui appelle le bon outil à partir d'un \"appel\" en JSON.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: l'appel produit par le LLM (doit correspondre à un outil existant, avec les bons arguments)\n",
    "        config: la configuration utilisée par LangChain, qui contient des trucs comme des callbacks, des métadonnées...\n",
    "\n",
    "    Returns:\n",
    "        sortie de l'outil\n",
    "    \"\"\"\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4_yWQXvp74N"
   },
   "outputs": [],
   "source": [
    "# Mettre à jour la chaîne pour avoir toutes les étapes\n",
    "chain = prompt | llm | JsonOutputParser() | invoke_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_TFOu5sqJLI"
   },
   "source": [
    "**L'heure de vérité :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I_Pz_MjqGuf"
   },
   "outputs": [],
   "source": [
    "print(f\"Problème : '{puzzle1}'\")\n",
    "print(f\"Réponse attendue : {1.95**10}\")\n",
    "print(f\"Réponse de la chaîne complète : {chain.invoke(puzzle1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPoQNrIzs8Ho"
   },
   "source": [
    "Et, pour un autre problème faisant intervenir une multiplication :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSXSsBC9tA8y"
   },
   "outputs": [],
   "source": [
    "puzzle2 = \"Si une tomate pèse 184.48376 grammes, combien pèsent 417453 tomates ?\"\n",
    "print(f\"Problème : '{puzzle2}'\")\n",
    "print()\n",
    "\n",
    "print(f\"Réponse attendue : {417453*184.48376:_}\")\n",
    "print()\n",
    "\n",
    "print(\"Réponse SANS les outils :\")\n",
    "invoke(llm, puzzle2, display_output = True)\n",
    "print()\n",
    "\n",
    "print(\"Réponse AVEC les outils :\")\n",
    "print(f\"{chain.invoke(puzzle2):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBEye7jqCZ6S"
   },
   "source": [
    "**À vous de jouer !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlxNWW1nC7oD"
   },
   "source": [
    "Pourquoi s'arrêter en si bon chemin ? Complétez le code pour ajouter un 4e outil permettant à la chaîne de résoudre le problème suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc43VYVMC6uX"
   },
   "outputs": [],
   "source": [
    "# Problème à résoudre\n",
    "puzzle3 = \"Combien vaut ln(456789451.32194846) ?\"\n",
    "print(f\"Problème : '{puzzle3}'\")\n",
    "print()\n",
    "\n",
    "# Solution correcte\n",
    "from math import log\n",
    "print(f\"Réponse attendue : {log(456789451.32194846)}\")\n",
    "print()\n",
    "\n",
    "# Réponse du LLM sans outils\n",
    "print(\"Réponse SANS les outils :\")\n",
    "invoke(llm, puzzle3, display_output = True)\n",
    "print()\n",
    "\n",
    "# Répose de la chaîne en l'état\n",
    "print(\"Réponse AVEC les outils ACTUELS :\")\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "try:\n",
    "  print(chain.invoke(puzzle3))\n",
    "except:\n",
    "  print(\"Le modèle n'a pas appelé d'outil correctement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voSEtulUEJAR"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# TODO : implémenter l'outil ln (inspirez-vous des outils déjà créés)\n",
    "@tool\n",
    "def ln(x: float) -> float:\n",
    "   \"\"\"Computes ln(x), i.e. raises the natural logarithm of the argument.\"\"\"\n",
    "   return log(x)\n",
    "\n",
    "# Ajouter l'outil\n",
    "tools = [add, multiply, exponentiate, ln]\n",
    "\n",
    "# Mettre à jour le prompt avec la nouvelle description des outils\n",
    "rendered_tools = render_text_description(tools)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", system_prompt.format(rendered_tools=rendered_tools)),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# Mettre à jour la chaîne\n",
    "chain = prompt | llm | JsonOutputParser() | invoke_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4ozeBTgK4OA"
   },
   "outputs": [],
   "source": [
    "# Afficher le résultat avec le nouvel outil\n",
    "print(\"Réponse AVEC TOUS LES OUTILS (add, multiply, exponentiate, ln) :\")\n",
    "print(chain.invoke(puzzle3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afi-CPgDupup"
   },
   "source": [
    "## 3) Notre premier agent & Introduction à LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAf4rZ4ju0S1"
   },
   "source": [
    "Jusqu'à présent nous utilisons des chaînes, qui sont conçues pour éxécuter une seule passe d'un chemin linéaire et déterministe ; en l'occurence :\n",
    "\n",
    "`Passer un prompt -> Produire une réponse -> Parser le JSON -> Retourner le retour de l'outil appelé`\n",
    "\n",
    "Cette architecture limitée ne permet pas de faire face à des cas d'usage plus complexes, nécessitant de définir un flux dynamique, avec des boucles, des conditions et des transitions entre plusieurs états.\n",
    "\n",
    "Par exemple, comment gérer le cas où le problème ne nécessite pas d'outils ou est mal posé ? Et que faire si le problème nécessite de calculer des résultats intermédiaires, et donc d'appeler les outils plusieurs fois, en cascade ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZgVo1TVxg0u"
   },
   "source": [
    "Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WeXvMYVxi72"
   },
   "outputs": [],
   "source": [
    "puzzle4 = \"À un taux d'intérêt annuel de 95 %, quelle serait la valeur de 417453 € dans 10 ans ?\"\n",
    "print(f\"Problème : {puzzle4}\")\n",
    "print()\n",
    "\n",
    "print(f\"Réponse attendue : {417453 * (1.95**10):_}\")\n",
    "print()\n",
    "\n",
    "print(\"Réponse de la CHAÎNE ACTUELLE :\")\n",
    "print(f\"{chain.invoke(puzzle4):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDyT4RhLyFdB"
   },
   "source": [
    "La chaîne a simplement produit un résultat intermédiaire (l'exponentiation), et n'est pas assez souple pour permettre au LLM de poursuivre son raisonnement (multiplier ce nombre par le montant initial, 417453)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQJKtBuFwyC9"
   },
   "source": [
    "Pour passer outre ces limitations, nous devons passer d'une chaîne à un vrai **agent**.\n",
    "\n",
    "Un agent LangGraph est une entité \"guidée\" par un LLM : en recevant une tâche, il analyse le contexte (la question et l’historique) et décide de l’action à effectuer. Il peut :\n",
    "- appeler l'un de ses outils,\n",
    "- transmettre la tâche à un autre agent (nous verrons cela plus tard),\n",
    "- donner une réponse finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNYKXgoUuzM6"
   },
   "outputs": [],
   "source": [
    "# Prompt système : encourage l'agent à agent à adopter une personnalité de mathématicien qui décompose les problèmes\n",
    "system_prompt = \"\"\"You are an expert mathematician and problem solver.\n",
    "When faced with a word problem (like finance, physics, etc.), your goal is to:\n",
    "1. Identify the mathematical formula required.\n",
    "2. Break down that formula into simple arithmetic steps (add, multiply, power).\n",
    "3. Use the available tools sequentially to solve it.\n",
    "\n",
    "Do not say you cannot do it because of the topic. If it involves numbers, you can solve it using your math tools.\"\"\"\n",
    "\n",
    "# Instancier l'agent\n",
    "# (à partir de maintenant, on utilisera la fonction create_agent qui crée un objet qui gère déjà le parsing et l'appel d'outils en cascade)\n",
    "from langchain.agents import create_agent\n",
    "agent = create_agent(llm, tools, system_prompt=system_prompt)\n",
    "\n",
    "# Fonction pour passer un problème à l'agent et retourner toutes les étapes\n",
    "def invoke_agent(agent, input, display_last_output = False):\n",
    "  response = agent.invoke({\"messages\": [(\"user\", puzzle4)]}) # contient toutes les réponses\n",
    "\n",
    "  if display_last_output: # Afficher le dernier message en markdown\n",
    "    display(Markdown(response[\"messages\"][-1].content))\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rkpp7jU_4cO"
   },
   "outputs": [],
   "source": [
    "# Même problème\n",
    "print(f\"Problème : {puzzle4}\")\n",
    "print()\n",
    "\n",
    "# Soumettre le problème à l'agent\n",
    "print(\"Réponse de l'agent:\")\n",
    "response = invoke_agent(agent, puzzle4, display_last_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTpBtSJhuxkj"
   },
   "source": [
    "Le bon résultat est maintenant trouvé !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFIVxiwEB7oJ"
   },
   "source": [
    "Pour comprendre comment l'agent créé par LangChain (via la fonction `create_agent`) fonctionne en interne, regardons l'historique des outils appelés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-4JXnHZCMpK"
   },
   "outputs": [],
   "source": [
    "# Affichage des étapes (pour comprendre le raisonnement)\n",
    "for message in response[\"messages\"]:\n",
    "  if message.type == \"ai\" and message.tool_calls:\n",
    "    print(f\"- L'IA décide d'appeler l'outil '{message.tool_calls[0]['name']}' avec {message.tool_calls[0]['args']}\")\n",
    "  elif message.type == \"tool\":\n",
    "    print(f\"\\t=> Résultat de l'outil : {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMoJiHXqC-2y"
   },
   "source": [
    "Pour avoir le détail des entrées/sorties du LLM à chaque étape, et avoir un aperçu clair de l'activité de nos agents (très utile dans des systèmes plus complexes), nous pouvons utiliser **LangSmith**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXv1CgUOhvR"
   },
   "source": [
    "Pour ce faire, créez un compte sur https://smith.langchain.com/ (vous pouvez le faire via votre compte Google, votre compte GitHub ou simplement par email).\n",
    "\n",
    "Dans le panneau de gauche, cliquez sur \"Settings\" puis \"API Keys\" et créez un \"Personal Access Token\" dans le \"Workspace 1\".\n",
    "\n",
    "Renseignez la clé ici, en exécutant cette cellule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3FcbHA1OhIh"
   },
   "outputs": [],
   "source": [
    "LANGSMITH_KEY = input()\n",
    "\n",
    "# Nettoyer la sortie de la cellule par précaution\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(f\"Clé enregistrée : {LANGSMITH_KEY[:2]}...{LANGSMITH_KEY[-2:]}\")\n",
    "\n",
    "# Configurer l'environnement\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGSMITH_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"TP LangGraph\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp_MGFlbaEOD"
   },
   "source": [
    "Ensuite, revenez à l'accueil et cliquez sur \"Tracing Projects\" et entrez dans le projet créé par défaut. Ici, vous aurez accès à toutes les invocations d'agents que vous effecturez, dès lors que vous activez le traçage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJn2YQBJQtEC"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "with tracing_v2_enabled():\n",
    "  response = invoke_agent(agent, puzzle4, display_last_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv2GLOEna1Je"
   },
   "source": [
    "La trace devrait maintenant être visible sur le site de LangSmith. En cliquant dessus, vous pouvoir voir le détail des entrées/sorties et appel d'outils à chaque étape de la résolution du problème, ainsi que les modèles, coûts et latences associés.\n",
    "Essayez de bien comprendre chaque étape de l'execution visible sous LangSmith et vérifiez que vous obtenez bien ce qui était prévu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU1DSXnYMWEh"
   },
   "source": [
    "LangSmith comporte plein d'autres fonctionnalités intéressantes, comme le déploiement et le monitoring d'applications LangChain/LangGraph : https://docs.langchain.com/langsmith/home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2aMvZbH5tBU"
   },
   "source": [
    "## 4) Notre première application LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1LZZ0JIcWi1"
   },
   "source": [
    "Nous avons vu comment construire **UN** agent fonctionnel avec plusieurs outils à sa disposition. Nous allons maintenant construire une réelle application LangGraph, multi-agents. C'est un graphe où :\n",
    "- 1 noeud = 1 agent (avec un ensemble d'outils à disposition)\n",
    "- 1 arête = 1 \"passage de main\" qu'un agent peut décider d'opérer vers un autre.\n",
    "\n",
    "=> **MAIS :** Pourquoi s'embêter à créer plusieurs agents, au lieu de simplement continuer d'ajouter davantage d'outils divers et variés (recherche internet, exécuteur de requêtes SQL...) à un unique agent, pour en faire un couteau suisse autosuffisant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVlhKd6cc692"
   },
   "source": [
    "Passer d'un agent à une application multi-agents est souvent préférable car :\n",
    "- On peut associer à chaque agent un prompt spécifique, pour que chacun soit spécialisé dans un aspect de l'application (ex : un expert des maths + un administrateur SQL + ...)\n",
    "- Le \"cloisonnement\" des tâches permet d'alléger le nombre d'outils et le contexte à fournir à chaque agent (=> réduction du coût et des hallucinations dues à la longueur des prompts)\n",
    "- Les agents peuvent agir en parallèle (=> diminution de la latence)\n",
    "- La structure en graphe facilite la mise en oeuvre de certains patrons de conception (_design pattern_), comme la **_reflection_** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtgOZHQdgJaH"
   },
   "source": [
    "![Alt text](https://www.baihezi.com/mirrors/langgraph/tutorials/reflection/img/reflection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-YXuLggM2TN"
   },
   "source": [
    "Nous allons créer une application LangGraph qui servira d'**assistant à des techniciens chargés de maintenir et réparer les différentes machines d'une usine.**\n",
    "\n",
    "L'assistant pourra :\n",
    "- consulter une base SQLite contenant les caractéristiques des machines et les journaux des précédentes interventions,\n",
    "- s'appuyer sur un corpus de guides d'utilisation des machines.\n",
    "\n",
    "Toutes les données (fictives) existent déjà, dans le dossier `resources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c58nR1TMo2r_"
   },
   "outputs": [],
   "source": [
    "from resources.utils import display_database\n",
    "\n",
    "display_database(\"resources/maintenance.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiYjFQvMB_zI"
   },
   "source": [
    "### 4.1) État partagé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHxJTMsFCMsj"
   },
   "source": [
    "Tout d'abord, définissons **l'état partagé**, une structure de donnée partagée entre les noeuds, qui représente l'état courant de l'application :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQIgFuD9CL5D"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    #TODO Ajoutez un attribut 'next' de type string. IL sera utilisé plus tard pour stocker le nom du prochain noeud.\n",
    "    next: str\n",
    "\n",
    "    #TODO Ajoutez un attribut 'messages', de type list[BaseMessage], avec un réducteur dont la fonction de réduction est 'operator.add' (voir cours).\n",
    "    messages: Annotated[list[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZQg6i_GDNeA"
   },
   "source": [
    "### 4.2) Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLxoAsJzx76l"
   },
   "source": [
    "Ensuite, créons les 3 agents de l'application, un par un :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNtlSVADnpoB"
   },
   "source": [
    "1) **Un agent SQL, qui peut lire, ajouter ou supprimer les données dans les 2 tables SQL suivantes :**\n",
    "```sql\n",
    "CREATE TABLE machines (\n",
    "    machine_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    model_name TEXT,\n",
    "    install_date TEXT,\n",
    "    end_of_warranty_date TEXT,\n",
    "    warranty_active BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE TABLE maintenance_logs (\n",
    "    log_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    FOREIGN KEY(machine_id) REFERENCES machines(machine_id),\n",
    "    date TEXT,\n",
    "    issue TEXT,\n",
    "    solution TEXT,\n",
    "    cost INTEGER\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J14oryulbJHE"
   },
   "source": [
    "Base de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SrMXE08Tipw"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlalchemy import create_engine, event\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# Chemin de la base de données (déjà créée !), depuis le notebook\n",
    "database_relative_path = \"resources/maintenance.db\"\n",
    "\n",
    "# Blacklistons certains types de requêtes pour empêcher l'agent de faire n'importe quoi\n",
    "def sqlite_authorizer(action_code, table, column, sql_main, trigger):\n",
    "    \"\"\"\n",
    "    Politique de sécurité \"Blacklist\" :\n",
    "    On autorise tout par défaut, sauf ce qui est explicitement interdit.\n",
    "    \"\"\"\n",
    "    # Codes d'action SQLite\n",
    "    SQLITE_DELETE = 9       # Suppression de lignes\n",
    "    SQLITE_DROP_TABLE = 11  # Suppression de table\n",
    "    SQLITE_ALTER_TABLE = 26 # Modification de structure\n",
    "    SQLITE_UPDATE = 23      # Mise à jour\n",
    "\n",
    "    # Supression de données, altération du schéma : interdit\n",
    "    if action_code in [SQLITE_DELETE, SQLITE_DROP_TABLE, SQLITE_ALTER_TABLE]:\n",
    "        return sqlite3.SQLITE_DENY\n",
    "\n",
    "    # Interdit de modifier des logs\n",
    "    if table == \"maintenance_logs\" and action_code == SQLITE_UPDATE:\n",
    "            return sqlite3.SQLITE_DENY\n",
    "\n",
    "    # On autorise le reste\n",
    "    return sqlite3.SQLITE_OK\n",
    "\n",
    "\n",
    "engine = create_engine(f\"sqlite:///{database_relative_path}\")\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def connect(dbapi_connection, connection_record):\n",
    "    \"\"\"\n",
    "    À chaque connexion physique à la DB, on active l'authorizer.\n",
    "    \"\"\"\n",
    "    # dbapi_connection est l'objet sqlite3 pur\n",
    "    dbapi_connection.set_authorizer(sqlite_authorizer)\n",
    "\n",
    "db = SQLDatabase(engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCJeRDKMbLn3"
   },
   "source": [
    "Outils nécessaires pour notre agent SQL (déjà implémentés par LangChain !) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMdKH-H3l_iF"
   },
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "print(\"Outils de l'agent SQL :\")\n",
    "for sql_tool in tools:\n",
    "  print(f\"- {sql_tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9GQRmbGbSUg"
   },
   "source": [
    "Sécurisons l'outil de requête SQL afin de faire valider toute requête autre que \"SELECT\" par l'utilisateur (Man In The Loop) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhgpaR123Es-"
   },
   "outputs": [],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "raw_sql_tool = tools[0]\n",
    "\n",
    "@tool\n",
    "def sql_db_query(query: str) -> str:\n",
    "  \"\"\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\"\"\"\n",
    "  is_read_only=query.strip().upper().startswith(\"SELECT\")\n",
    "\n",
    "  if not is_read_only:\n",
    "        # Cela met le graphe en pause et renvoie la valeur à l'utilisateur\n",
    "        # L'utilisateur devra répondre pour reprendre l'exécution (Man in the loop)\n",
    "        human_review = interrupt({\n",
    "            \"query\": query,\n",
    "            \"warning\": \"Cette requête va modifier la base de données. Approuver ?\"\n",
    "        })\n",
    "\n",
    "        action = human_review.get(\"action\", \"reject\")\n",
    "\n",
    "        if action != \"approve\":\n",
    "            return f\"Action rejetée par l'utilisateur. La requête '{query}' n'a pas été exécutée.\"\n",
    "\n",
    "  try:\n",
    "      return raw_sql_tool.invoke(query)\n",
    "  except Exception as e:\n",
    "      return f\"Erreur SQL: {e}\"\n",
    "\n",
    "tools[0] = sql_db_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXqLCTu8bViT"
   },
   "source": [
    "Créer l'agent SQL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBlMprgb_Uku"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "sql_system_prompt = \"\"\"# CONTEXTE\n",
    "Tu es admin d'une base de données SQLite, qui contient des informations sur les machines de l'usine.\n",
    "\n",
    "# SCHÉMA RELATIONNEL\n",
    "Tu as accès aux tables suivantes (noms d'attributs en anglais, contenu en français) :\n",
    "  - machines (machine_id INTEGER, model_name TEXT, install_date TEXT, end_of_warranty_date TEXT, warranty_active BOOLEAN)\n",
    "  - maintenance_logs (log_id INTEGER, machine_id FOREIGN KEY, date TEXT, issue TEXT, solution TEXT, cost INTEGER)\n",
    "\n",
    "# WORKFLOW\n",
    "1) Commence par déterminer où se trouvent les informations dont tu as besoin.\n",
    "2) Ensuite, écris puis exécute la ou les requêtes SQL appropriées\n",
    "- Quand tu filtres via un string fourni par l'utilisateur (ex : model_name), sois permissif : utilise 'LIKE' et ne sois PAS sensible à la casse.\n",
    "- Pour trouver le machine_id à partir du model_name, n'utilise pas 'LIMIT 1', pour t'assurer qu'il n'y a pas de doublons.\n",
    "3) Enfin, rédige tes conclusions de façon concise :\n",
    "- Ne donne que les informations demandées, pas le résultat complet de tes requêtes.\n",
    "- Sois transparent si tu ne trouves pas certaines données demandées. N'invente rien, si une requête échoue ou ne renvoie rien, dis-le.\n",
    "- Relève toute ambiguïté (ex : tu devais identifier une machine appelée \"Presse hydraulique\" et plusieurs machine_id correspondent à ce model_name).\n",
    "\"\"\"\n",
    "#TODO créez l'agent sql_agent avec les bons paramètres. Pour cela utlisez create_agent.\n",
    "sql_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=sql_system_prompt\n",
    ")\n",
    "\n",
    "# Encapsuler l'agent dans un noeud\n",
    "def sql_agent_node(state: AgentState):\n",
    "  result = sql_agent.invoke(state)\n",
    "\n",
    "  # On récupère le dernier message (ie le rapport que fait l'agent SQL au superviseur)\n",
    "  last_message = result[\"messages\"][-1]\n",
    "\n",
    "  return {\n",
    "      \"messages\": [AIMessage(content=last_message.content, name=\"Troubleshooter\")]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzEgDyJG12Pz"
   },
   "source": [
    "2) **Un agent RAG, qui peut exploiter un corpus de documents textuels (guides d'utilisation des machines) :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wROqzEvzjkxW"
   },
   "outputs": [],
   "source": [
    "# Dépendance nécessaire pour indexer la base de données pour faire du RAG\n",
    "%pip install --quiet langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tQeaCbVbaBC"
   },
   "source": [
    "Création d'une base de données en mémoire (dans la vraie vie, on utiliserait bien-sûr une base de données externe au lieu de tout charger en mémoire) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64oSBYCBkxPs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Chemin des documents\n",
    "DOCS_PATH = \"./resources/user_guides\"\n",
    "\n",
    "# Chemin du modèle d'embedding (permet d'indexer la base RAG)\n",
    "LOCAL_MODEL_PATH = \"./resources/embedding_model\"\n",
    "\n",
    "# Chargement des documents\n",
    "loader = DirectoryLoader(DOCS_PATH, glob=\"**/*\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Essaie de couper aux paragraphes d'abord\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embedding (on utilise le modèle déjà téléchargé dans ./resources/embedding_model)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=LOCAL_MODEL_PATH,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "# Instanciation du vector store en mémoire\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrZXo3pVbi4R"
   },
   "source": [
    "Outil de RAG :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oQDB-6zk9Hp"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_technical_docs(query: str):\n",
    "    \"\"\"\n",
    "    Search the technical documentation for machines (manuals, procedures, safety).\n",
    "    Use this tool to find:\n",
    "    - Repair protocols\n",
    "    - Error codes\n",
    "    - Safety instructions\n",
    "    - Technical specifications\n",
    "\n",
    "    Args:\n",
    "        query: The question or search keywords, in French (e.g., “comment changer joint presse hydraulique”)\n",
    "    \"\"\"\n",
    "    # Recherche des 3 documents les plus pertinents\n",
    "    results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "    # Construction de la réponse formatée pour le LLM\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(results):\n",
    "        source = doc.metadata.get('source', 'Inconnu').split('/')[-1]\n",
    "        context += f\"\\n--- EXTRAIT {i+1} (Source: {source}) ---\\n{doc.page_content}\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h5aiwqHbkuD"
   },
   "source": [
    "Créer l'agent RAG :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8IbJhV92ErG"
   },
   "outputs": [],
   "source": [
    "rag_system_prompt = \"\"\"Tu es spécialiste en documentation technique pour la maintenance de machines en usine.\n",
    "Ton rôle est d'aider les techniciens en recherchant des informations précises dans les guides d'utilisation des machines.\n",
    "\n",
    "Directives :\n",
    "- Utilise **TOUJOURS** l'outil pour répondre aux questions techniques. Ne te fie pas à tes connaissances internes.\n",
    "- Concentre-toi sur l'**exécution** des tâches (comm \"Comment remplacer le ventilateur ?\") et non sur le diagnostic (comme \"Pourquoi y a-t-il une surchauffe ?\").\n",
    "- Si l'outil renvoie des avertissements de sécurité (par exemple, LOTO, haute tension), mentionne-les explicitement en premier lieu.\n",
    "- Si le document ne contient pas la réponse, indique clairement que la procédure ne figure pas dans le manuel.\n",
    "\"\"\"\n",
    "#TODO créez l'agent rag_agent avec les bons paramètres. Pour cela utlisez create_agent.\n",
    "rag_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_technical_docs],\n",
    "    system_prompt=rag_system_prompt\n",
    ")\n",
    "\n",
    "# Encapsuler l'agent dans un noeud\n",
    "def rag_agent_node(state: AgentState):\n",
    "    result = rag_agent.invoke(state)\n",
    "    last_message = result[\"messages\"][-1]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=last_message.content, name=\"Technician\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOWDiWPvBqli"
   },
   "source": [
    "3) **Un agent superviseur, chargé d'orchestrer les recherches à partir des instructions de l'utilisateur :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNTz04uZ8s-c"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Annotated\n",
    "import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Classe de sortie (permet de produire une consigne de routage ou une réponse finale)\n",
    "class SupervisorDecision(TypedDict):\n",
    "    next: Literal[\"Troubleshooter\", \"Technician\", \"FINISH\"]\n",
    "    instructions: str  # Contiendra soit la consigne pour l'agent, soit la réponse finale\n",
    "\n",
    "# Prompt\n",
    "current_date = datetime.date.today().isoformat()\n",
    "\n",
    "supervisor_system_prompt = f\"\"\"# CONTEXTE\n",
    "Tu es le **l'assistant IA de réparation** d'une usine qui contient des machines à maintenir.\n",
    "En tant que chatbot, ton objectif est d'aider le technicien humain qui t'interroge pour te demander des informations sur la machine (diagnostic, procédure de réparation...).\n",
    "Date actuelle : {current_date}\n",
    "\n",
    "# AGENTS\n",
    "Pour ce faire, tu as 2 autres agents à ta disposition :\n",
    "1. **Troubleshooter** (agent SQL) : a accès à la base de données. Il peut te donner l'ID d'une machine à partir de son nom, ses informations de garantie et ses journaux de maintenance (problèmes rencontrés et solutions).\n",
    "2. **Technician** (agent RAG) : a accès aux manuels d'utilisation des machines. Il peut t'expliquer comment retirer/remplacer une machine et les éventuelles consignes de sécurité.\n",
    "\n",
    "# RÈGLES DE ROUTAGE (A SUIVRE RIGOUREUSEMENT)\n",
    "- Quoi qu'il arrive, ton but est de produire une structure JSON avec les champs `next` et `instructions`, pas du texte libre.\n",
    "- Analyse la conversation et indique le prochain acteur dans le champ `next` : 'Troubleshooter' ou 'Technician'. Donne lui les instructions/contexte nécessaire à sa tâche dans le champ `instructions`.\n",
    "\n",
    "**CAS SPÉCIAL : RÉPONSE FINALE**\n",
    "Si tu as recueilli toutes les informations nécessaires, veut demander des clarifications à l'utilisateur ou répond à une question non technique (ex : \"Qui es-tu ?\") :\n",
    "  1. Choisis impérativement **'FINISH'** pour le champ `next`.\n",
    "  2. Écris ta réponse à l'utilisateur (ex: \"Je suis l'assistant de maintenance...\") DANS le champ `instructions`.\n",
    "\n",
    "# MÉTHODOLOGIE\n",
    "Suis ce workflow :\n",
    "\n",
    "**ÉTAPE 1 : CLARIFIER OU INFORMER**\n",
    "- L'utilisateur peut ne rien demander (ex : 'Bonjour !') ou être trop vague (ex : ne pas citer quelle machine a un problème ou comment la panne se manifeste). Si c'est le cas, fais `FINISH` et demande des clarifications.\n",
    "\n",
    "**ÉTAPE 2 : IDENTIFIER LA MACHINE**\n",
    "- Demande au `Troubleshooter` :\n",
    "  - Si l'utilisateur a déjà fourni le machine_id, de vérifier qu'il existe et que les informations sur cette machine correspondent bien à celles décrites par l'utilisateur.\n",
    "  - Si l'utilisateur n'a fourni qu'un nom de machine (ex : 'Presse hydraulique'), de trouver le machine_id correspondant.\n",
    "- Si nécessaire, fais `FINISH` et demande des clarifications (ex : si plusieurs IDs correspondent au nom de machine fourni par l'utilisateur, demander des informations supplémentaires pour lever l'ambiguïté).\n",
    "\n",
    "**ÉTAPE 3 : DIAGNOSTIC**\n",
    "- Une fois la machine identifiée, demande au `Troubleshooter` de :\n",
    "  - Vérifier si la machine est actuellement sous garantie\n",
    "  - Regarder si des symptômes similaires sont mentionnés dans les 5 derniers journaux de maintenance de cette machine\n",
    "- Enfin, identifie la cause probable et la solution standard sur la base des journaux passés.\n",
    "\n",
    "**ÉTAPE 4 : EXÉCUTION ET SÉCURITÉ**\n",
    "Si une réparation est nécessaire (ex : \"Remplacer le joint torique\"), demande au `Technician` de trouver :\n",
    "  - Les avertissements de sécurité spécifiques (LOTO, EPI).\n",
    "  - La procédure de remplacement étape par étape pour ce modèle spécifique.\n",
    "\n",
    "**ÉTAPE 5 : CONCLUSIONS**\n",
    "- Fais un rapport clair et profesionnel sur tes conclusions (dans la langue de l'utilisateur) : ID de la machine, diagnostic, procédure de réparation, consignes de sécurité.\n",
    "- Sois transparent sur tes incertitudes voire les informations non trouvées (ex : procédure pour réparer la machine non présente dans le manuel).\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? \"\n",
    "            \"Select 'Troubleshooter', 'Technician', or 'FINISH' in the `next` field.\"\n",
    "            \"Write your instructions or final answer in the 'instructions' field.\"\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "# Contenu du noeud Supervisor : chaîne \"Prompt -> LLM (Format JSON forcé)\"\n",
    "supervisor_chain = prompt | llm.bind_tools([SupervisorDecision], tool_choice=\"auto\")\n",
    "\n",
    "def supervisor_node(state: AgentState):\n",
    "    decision = supervisor_chain.invoke({\"messages\": state[\"messages\"]})\n",
    "\n",
    "    # 1er cas : le LLM a produit un JSON comme prévu\n",
    "    if decision.tool_calls:\n",
    "      # On renvoie la décision pour le routing ET on ajoute le message du superviseur à l'historique\n",
    "      tool_args = decision.tool_calls[0][\"args\"]\n",
    "      return {\n",
    "          \"next\": tool_args.get(\"next\"),\n",
    "          \"messages\": [AIMessage(content=tool_args.get(\"instructions\"), name=\"Supervisor\")]\n",
    "      }\n",
    "\n",
    "    # 2e cas : le LLM a produit du texte brut\n",
    "    else:\n",
    "      return {\n",
    "          \"next\": \"FINISH\",\n",
    "          \"messages\": [AIMessage(content=decision.content, name=\"Supervisor\")]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyBSTw85EDZi"
   },
   "source": [
    "### 4.3) Création de l'application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QKy_yf5EDz8"
   },
   "source": [
    "Pour finir, créons l'application elle-même, en définissant sa typologie (noeuds, transitions). Voici le graphe que l'on souhaite obtenir :\n",
    "![graphe.png](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgwuUZjpFudY"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1IqSe0oEKec"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Créer un graphe vide avec notre état partagé\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Noeuds\n",
    "#TODO créer les noeuds Supervisor, Troubleshooter et Technician\n",
    "workflow.add_node(\"Supervisor\", supervisor_node)\n",
    "workflow.add_node(\"Troubleshooter\", sql_agent_node)\n",
    "workflow.add_node(\"Technician\", rag_agent_node)\n",
    "\n",
    "# Arêtes (transitions entre les agents)\n",
    "# TODO relier les noeuds entre eux\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "workflow.add_edge(\"Troubleshooter\", \"Supervisor\")\n",
    "workflow.add_edge(\"Technician\", \"Supervisor\")\n",
    "\n",
    "# Logique de sortie du superviseur\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    lambda state: END if state[\"next\"] == \"FINISH\" else state[\"next\"],\n",
    "     {\n",
    "         \"Troubleshooter\": \"Troubleshooter\",\n",
    "         \"Technician\": \"Technician\",\n",
    "         END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Création d'une mémoire qui stockera l'état de l'application\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compilation de l'application\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Afficher le graphe pour visualiser le workflow de l'application\n",
    "from IPython.display import display, Image\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyHYECTQwdtx"
   },
   "source": [
    "Vérifiez que le graphe obtenu correspond à celui énoncé plus haut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ohs15ZSFGi5"
   },
   "source": [
    "### 4.4) Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDfOspWwIXNp"
   },
   "source": [
    "C'est le moment de tester l'application ! Une fonction pour afficher de façon jolie une session interactive a déjà été codée dans `resources.utils` .\n",
    "\n",
    "N'hésitez pas à jouer avec l'application, à tester différents scénarios/entrées :\n",
    "- Essayer de demander des informations sur le contexte : `\"Je suis nouveau dans l'usine. Qui es-tu ?\"`\n",
    "- Essayer d'utiliser l'assistant pour réparer une machine (cas d'usage principal de l'application) : `\"La presse hydraulique fuit. Comment la réparer ?\"`\n",
    "- Essayer de demander à l'assistant d'ajouter des données à la base de données pour vérifier que le _Man in the loop_ fonctionne : `\"J'ai une nouvelle machine, un extincteur, garanti deux ans. Ajoute-le à la base de données SQL.\"`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16PsTPqvC3Xi"
   },
   "outputs": [],
   "source": [
    "from resources.utils import run_app_with_rich\n",
    "\n",
    "run_app_with_rich(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-coFbvdO5vN8"
   },
   "source": [
    "## 5) À vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHHsrjq3IixI"
   },
   "source": [
    "Dans cette partie, vous allez compléter l'application LangGraph en ajoutant quelques fonctionnalités de votre choix !\n",
    "\n",
    "**Quelques idées :**\n",
    "1) Équiper l'assistant d'un **moteur de recherche en ligne** (comme DuckDuckGo), pour compléter les données internes (RAG, SQL) par des infos externes (par exemple : le prix d'une pièce de rechange).\n",
    "2) Fournir à l'assistant un outil de recherche sur **Wikipedia**, pour trouver davantage d'infos techniques.\n",
    "3) Permettre à l'assistant d'**envoyer un message (discord, mail...)** au technicien humain, contenant un rapport qui synthétise les conclusions de l'assistant.\n",
    "4) Générer un **graphique** (par exemple : le coût des réparations au cours du temps)\n",
    "\n",
    "Pour rappel, de nombreux outils et ensembles d'outils (_toolkits_) sont déjà implémentés dans **`langchain_community`** : https://docs.langchain.com/oss/python/integrations/tools\n",
    "\n",
    "Pour vous lancer, nous vous proposons un squelette de code pour intégrer une fonctionnalité de recherche sur Wikipedia (c'est l'idée n°2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocMzUosGUECI"
   },
   "source": [
    "### 5.1) 3e fonctionnalité : Recherche sur Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBGtqPVrUeA-"
   },
   "source": [
    "On va permettre à l'assistant de chercher en ligne des informations techniques précises sur les machines, pièces... à la demande de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvbxXeJNWZcU"
   },
   "source": [
    "Pour ce faire, on va utiliser l'outil `WikipediaQueryRun` déjà implémenté dans `langchain_community` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD3KEP7fWndQ"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkTLYYrEXsfA"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# TODO : éventuellement, revenir ici ultérieurement pour tuner les paramètres\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(\n",
    "    lang=\"fr\", # la langue des pages à considérer\n",
    "    top_k_results=1, # le nombre de pages dont l'outil affiche le résumé (section en haut de la page)\n",
    "    doc_content_chars_max=1_000 # rogner le résultat de l'outil (concaténation des résumés des top_k_results premières pages) pour éviter de consommer tous nos crédits\n",
    "    ))\n",
    "\n",
    "@tool\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    \"\"\"Recherche une information sur Wikipédia en français et retourne un résumé concis.\"\"\"\n",
    "    return wikipedia_tool.invoke(query)\n",
    "\n",
    "print(wikipedia_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPOitvay_daU"
   },
   "outputs": [],
   "source": [
    "# Testons l'outil\n",
    "print(wikipedia_tool.invoke(\"Chopin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9mjycEsWvZI"
   },
   "source": [
    "Création d'un noeud `WikipediaAgent` dédié à cette fonctionnalité. L'idée est que, sachant que cet agent a accès à Wikipedia, le `Supervisor` puisse décider de lui poser une question technique (par exemple : \"Quels sont les 3 standards de dureté Shore ?\") ; le `WikipediaAgent` s'occupe de déterminer quelle requête envoyer à Wikipedia et d'extraire/synthétiser l'information pour le `Supervisor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUzdwtQRW9VT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# TODO : rédiger le prompt (contexte, consignes, guidelines).\n",
    "# Conseil : insister sur le fait de toujours utiliser l'outil pour répondre, sans rien inventer, et d'être transparent sur les informations non trouvées\n",
    "wikipedia_system_prompt = \"\"\"TODO\n",
    "# CONTEXTE\n",
    "Tu es un assistant expert en recherche d'informations sur Wikipédia.\n",
    "# RÈGLES\n",
    "- Utilise **TOUJOURS** l'outil pour répondre aux questions factuelles. Ne te fie pas à tes connaissances internes.\n",
    "- Si l'outil renvoie une page Wikipédia, résume les informations clés de manière concise.\n",
    "- Si l'outil ne trouve pas d'information pertinente, indique-le clairement.\n",
    "# GUIDELINES\n",
    "- Sois précis et concis dans tes réponses.\n",
    "- Cite tes sources si possible.\n",
    "\"\"\"\n",
    "\n",
    "# Création de l'agent\n",
    "wikipedia_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[wikipedia_search],\n",
    "    system_prompt=wikipedia_system_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# TODO : Création du noeud encapsulant l'agent (s'inspirer de l'agent SQL ou de l'agent RAG)\n",
    "def wikipedia_agent_node(state: AgentState):\n",
    "  result = wikipedia_agent.invoke(state)\n",
    "  last_message = result[\"messages\"][-1]\n",
    "  return {\n",
    "      \"messages\": [AIMessage(content=last_message.content, name=\"WikiExpert\")]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfQ5Yq7kZvgR"
   },
   "source": [
    "Mise à jour du superviseur pour inclure cet agent (expliquer dans le prompt ce qu'il fait et quand l'appeler, mettre à jour les consignes de routage...) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsqtALrSaENZ"
   },
   "outputs": [],
   "source": [
    "# Ceci est un copier-coller du code du superviseur, de la partie 4.2.\n",
    "# Suivez simplement les TODOs\n",
    "\n",
    "from typing import TypedDict, Literal, Annotated\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class SupervisorDecision(TypedDict):\n",
    "    next: Literal[\"Troubleshooter\", \"Technician\", \"WikiExpert\", \"FINISH\"] # TODO : ajouter l'agent\n",
    "    instructions: str\n",
    "\n",
    "# Prompt\n",
    "# TODO : ajuster le prompt pour inclure l'agent\n",
    "supervisor_system_prompt = f\"\"\"# CONTEXTE\n",
    "Tu es le **l'assistant IA de réparation** d'une usine qui contient des machines à maintenir.\n",
    "En tant que chatbot, ton objectif est d'aider le technicien humain qui t'interroge pour te demander des informations sur la machine (diagnostic, procédure de réparation...).\n",
    "Date actuelle : {current_date}\n",
    "\n",
    "# AGENTS\n",
    "Pour ce faire, tu as 3 autres agents à ta disposition :\n",
    "1. **Troubleshooter** (agent SQL) : a accès à la base de données. Il peut te donner l'ID d'une machine à partir de son nom, ses informations de garantie et ses journaux de maintenance (problèmes rencontrés et solutions).\n",
    "2. **Technician** (agent RAG) : a accès aux manuels d'utilisation des machines. Il peut t'expliquer comment retirer/remplacer une machine et les éventuelles consignes de sécurité.\n",
    "3. **WikiExpert** (agent Wikipedia) : a accès à Wikipédia pour rechercher des informations générales sur des concepts techniques, des pièces de machines ou des procédures de maintenance.\n",
    "\n",
    "# RÈGLES DE ROUTAGE (A SUIVRE RIGOUREUSEMENT)\n",
    "- Quoi qu'il arrive, ton but est de produire une structure JSON avec les champs `next` et `instructions`, pas du texte libre.\n",
    "- Analyse la conversation et indique le prochain acteur dans le champ `next` : 'Troubleshooter', 'Technician' ou 'WikiExpert'. Donne lui les instructions/contexte nécessaire à sa tâche dans le champ `instructions`.\n",
    "\n",
    "**CAS SPÉCIAL : RÉPONSE FINALE**\n",
    "Si tu as recueilli toutes les informations nécessaires, veut demander des clarifications à l'utilisateur ou répond à une question non technique (ex : \"Qui es-tu ?\") :\n",
    "  1. Choisis impérativement **'FINISH'** pour le champ `next`.\n",
    "  2. Écris ta réponse à l'utilisateur (ex: \"Je suis l'assistant de maintenance...\") DANS le champ `instructions`.\n",
    "\n",
    "# MÉTHODOLOGIE\n",
    "Suis ce workflow :\n",
    "\n",
    "**ÉTAPE 1 : CLARIFIER OU INFORMER**\n",
    "- L'utilisateur peut ne rien demander (ex : 'Bonjour !') ou être trop vague (ex : ne pas citer quelle machine a un problème ou comment la panne se manifeste). Si c'est le cas, fais `FINISH` et demande des clarifications.\n",
    "- L'utilisateur peut poser des questions techniques sur des concepts, pièces ou procédures. Si c'est le cas, fais `FINISH` et utilise l'agent `WikiExpert` pour répondre à la question.\n",
    "\n",
    "**ÉTAPE 2 : IDENTIFIER LA MACHINE**\n",
    "- Demande au `Troubleshooter` :\n",
    "  - Si l'utilisateur a déjà fourni le machine_id, de vérifier qu'il existe et que les informations sur cette machine correspondent bien à celles décrites par l'utilisateur.\n",
    "  - Si l'utilisateur n'a fourni qu'un nom de machine (ex : 'Presse hydraulique'), de trouver le machine_id correspondant.\n",
    "- Si nécessaire, fais `FINISH` et demande des clarifications (ex : si plusieurs IDs correspondent au nom de machine fourni par l'utilisateur, demander des informations supplémentaires pour lever l'ambiguïté).\n",
    "\n",
    "**ÉTAPE 3 : DIAGNOSTIC**\n",
    "- Une fois la machine identifiée, demande au `Troubleshooter` de :\n",
    "  - Vérifier si la machine est actuellement sous garantie\n",
    "  - Regarder si des symptômes similaires sont mentionnés dans les 5 derniers journaux de maintenance de cette machine\n",
    "  - Si nécessaire, demander au `WikiExpert` des informations complémentaires sur des concepts techniques ou des pièces spécifiques mentionnées dans les journaux.\n",
    "- Enfin, identifie la cause probable et la solution standard sur la base des journaux passés.\n",
    "\n",
    "**ÉTAPE 4 : EXÉCUTION ET SÉCURITÉ**\n",
    "Si une réparation est nécessaire (ex : \"Remplacer le joint torique\"), demande au `Technician` de trouver :\n",
    "  - Les avertissements de sécurité spécifiques (LOTO, EPI).\n",
    "  - La procédure de remplacement étape par étape pour ce modèle spécifique.\n",
    "\n",
    "**ÉTAPE 5 : CONCLUSIONS**\n",
    "- Fais un rapport clair et profesionnel sur tes conclusions (dans la langue de l'utilisateur) : ID de la machine, diagnostic, procédure de réparation, consignes de sécurité.\n",
    "- Sois transparent sur tes incertitudes voire les informations non trouvées (ex : procédure pour réparer la machine non présente dans le manuel).\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? \"\n",
    "            \"Select 'Troubleshooter', 'Technician', 'WikiExpert' or 'FINISH' in the `next` field.\" # TODO : ajouter l'agent\n",
    "            \"Write your instructions or final answer in the 'instructions' field.\"\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "# Chaine\n",
    "supervisor_chain = prompt | llm.bind_tools([SupervisorDecision], tool_choice=\"auto\")\n",
    "\n",
    "# Noeud\n",
    "def supervisor_node(state: AgentState):\n",
    "    decision = supervisor_chain.invoke({\"messages\": state[\"messages\"]})\n",
    "\n",
    "    if decision.tool_calls:\n",
    "      tool_args = decision.tool_calls[0][\"args\"]\n",
    "      return {\n",
    "          \"next\": tool_args.get(\"next\"),\n",
    "          \"messages\": [AIMessage(content=tool_args.get(\"instructions\"), name=\"Supervisor\")]\n",
    "      }\n",
    "\n",
    "    else:\n",
    "      return {\n",
    "          \"next\": \"FINISH\",\n",
    "          \"messages\": [AIMessage(content=decision.content, name=\"Supervisor\")]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-8xwZa3Z7RU"
   },
   "source": [
    "Mise à jour de l'application pour inclure l'agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eulnBXwNWrk_"
   },
   "outputs": [],
   "source": [
    "# Recréer le workflow avec le nouveau noeud Supervisor et le noeud Wikipedia\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Supervisor\", supervisor_node)\n",
    "workflow.add_node(\"Troubleshooter\", sql_agent_node)\n",
    "workflow.add_node(\"Technician\", rag_agent_node)\n",
    "\n",
    "# TODO : ajouter le noeud au workflow\n",
    "workflow.add_node(\"WikiExpert\", wikipedia_agent_node)\n",
    "\n",
    "# TODO : ajouter les bonnes arêtes au workflow\n",
    "workflow.add_edge(\"WikiExpert\", \"Supervisor\")\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "workflow.add_edge(\"Troubleshooter\", \"Supervisor\")\n",
    "workflow.add_edge(\"Technician\", \"Supervisor\")\n",
    "\n",
    "# TODO : inclure l'agent dans le conditional_edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    lambda state: END if state[\"next\"] == \"FINISH\" else state[\"next\"],\n",
    "     {\n",
    "         \"Troubleshooter\": \"Troubleshooter\",\n",
    "         \"Technician\": \"Technician\",\n",
    "         \"WikiExpert\": \"WikiExpert\",\n",
    "         END: END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx5hVWPxcF5F"
   },
   "source": [
    "On réutilise le code précédent pour compiler l'application :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF0yEjB1b_xE"
   },
   "outputs": [],
   "source": [
    "# Création d'une mémoire qui stockera l'état de l'application\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compilation de l'application\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Afficher le graphe pour visualiser le workflow de l'application\n",
    "from IPython.display import display, Image\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGE5nNnXcH2n"
   },
   "source": [
    "Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRXlCYZkcKCC"
   },
   "outputs": [],
   "source": [
    "from resources.utils import run_app_with_rich\n",
    "\n",
    "run_app_with_rich(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbsR7aDaWXJa"
   },
   "source": [
    "**Note :** Il est possible que cela ne marche pas du premier coup et qu'il faille par exemple :\n",
    "- tuner les paramètres de l'outil (ex : nombre de pages retournées par la recherche),\n",
    "- tuner les prompts du `WikipediaAgent` et/ou du `Supervisor`, notamment s'ils hallucinent ou ne formatent pas leurs retours correctement (ce qui entraîne une erreur 400),\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMAR0rTHdNfR"
   },
   "source": [
    "### 5.2) 4e fonctionnalité : à déterminer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0g3kHKPdh2m"
   },
   "source": [
    "Si vous êtes inspiré, reprenez ces mêmes principes pour ajouter un feature de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDd7bBa0qSVF"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxZh6zEJxMyC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UFQZGt3fQuu9",
    "6TVBYjj2b8Eg",
    "Afi-CPgDupup",
    "j2aMvZbH5tBU",
    "xiYjFQvMB_zI",
    "3ZQg6i_GDNeA",
    "UyBSTw85EDZi",
    "9Ohs15ZSFGi5",
    "-coFbvdO5vN8"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
