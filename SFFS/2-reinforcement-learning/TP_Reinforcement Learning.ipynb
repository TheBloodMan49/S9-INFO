{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bf60c1-2f2a-4314-aa68-e5de26914c6e",
   "metadata": {
    "id": "97bf60c1-2f2a-4314-aa68-e5de26914c6e"
   },
   "source": [
    "# TP Reinforcement Learning : Programmation Dynamique et MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364911c7-c380-4b9e-977c-6da1c3cfc7e3",
   "metadata": {
    "id": "364911c7-c380-4b9e-977c-6da1c3cfc7e3"
   },
   "source": [
    "## Objectifs du TP\n",
    "Ce TP vous permettra de :\n",
    "- Comprendre et implémenter les concepts fondamentaux des MDP (Processus de Décision Markoviens)\n",
    "- Mettre en pratique les algorithmes de programmation dynamique (Policy Evaluation, Policy Iteration, Value Iteration)\n",
    "- Analyser l'impact des hyperparamètres (facteur d'actualisation γ, seuil de convergence θ)\n",
    "- Visualiser et interpréter les résultats\n",
    "\n",
    "## Environnement : FrozenLake 8x8\n",
    "Vous allez travailler avec l'environnement FrozenLake de Gymnasium :\n",
    "- **S** : Position de départ (Start)\n",
    "- **F** : Surface gelée - sûre (Frozen)\n",
    "- **H** : Trou - fin du jeu (Hole)\n",
    "- **G** : Objectif - récompense +1 (Goal)\n",
    "\n",
    "Actions possibles : 0=Gauche, 1=Bas, 2=Droite, 3=Haut\n",
    "\n",
    "## Rappels du cours :\n",
    "1. MDP défini par (S, A, P, R, γ)\n",
    "2. Équation de Bellman pour V^π(s) et V*(s)\n",
    "3. Policy Evaluation : calculer V^π pour une politique π donnée\n",
    "4. Policy Improvement : améliorer π en choisissant l'action qui maximise Q(s,a)\n",
    "5. Policy Iteration : alterner évaluation et amélioration\n",
    "6. Value Iteration : combiner évaluation et amélioration en une seule mise à jour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6feaa1-b10c-4593-801e-9bfd1fa6afdd",
   "metadata": {
    "id": "0d6feaa1-b10c-4593-801e-9bfd1fa6afdd"
   },
   "source": [
    "## Installation et configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "db3bc647-f241-4301-853b-4b0fb2f94f1e",
   "metadata": {
    "id": "db3bc647-f241-4301-853b-4b0fb2f94f1e"
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"CRÉATION DE L'ENVIRONNEMENT FROZENLAKE 8x8\")\n",
    "\n",
    "# Créer l'environnement\n",
    "env = gym.make('FrozenLake-v1', map_name='8x8', is_slippery=False)\n",
    "\n",
    "# Afficher les informations\n",
    "lake_map = [\n",
    "    \"SFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFHFFFF\",\n",
    "    \"FFFFFHFF\",\n",
    "    \"FFFHFFFF\",\n",
    "    \"FHHFFFHF\",\n",
    "    \"FHFFHFHF\",\n",
    "    \"FFFHFFFG\"\n",
    "]\n",
    "\n",
    "print(\"\\nCarte du FrozenLake :\")\n",
    "for row in lake_map:\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\nNombre d'états : {env.observation_space.n}\")\n",
    "print(f\"Nombre d'actions : {env.action_space.n}\")\n",
    "print(\"\\nActions : 0=Gauche, 1=Bas, 2=Droite, 3=Haut\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c55de5e-54d1-490f-bd67-057dd9be9282",
   "metadata": {
    "id": "0c55de5e-54d1-490f-bd67-057dd9be9282"
   },
   "source": [
    "## Partie 1 : Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ff0b1d4-b91d-45d7-9f2b-56ae109e621b",
   "metadata": {
    "id": "3ff0b1d4-b91d-45d7-9f2b-56ae109e621b"
   },
   "source": [
    "def policy_evaluation(env, policy, discount_factor=0.9, theta=1e-5):\n",
    "    \"\"\"\n",
    "    Évalue une politique donnée en calculant sa fonction de valeur V^π(s).\n",
    "\n",
    "    Rappel du cours :\n",
    "    V^π(s) = Σ_a π(a|s) * Σ_{s',r} p(s',r|s,a) * [r + γ * V^π(s')]\n",
    "\n",
    "    Args:\n",
    "        env: L'environnement Gym\n",
    "        policy: Politique à évaluer (matrice |S| x |A|)\n",
    "        discount_factor: Facteur d'actualisation γ\n",
    "        theta: Seuil de convergence\n",
    "\n",
    "    Returns:\n",
    "        V: Fonction de valeur (vecteur de taille |S|)\n",
    "    \"\"\"\n",
    "    # TODO : Initialiser V(s) à zéro pour tous les états\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        # TODO 2: Pour chaque état s\n",
    "        for s in range(len(V)):\n",
    "            v = 0\n",
    "\n",
    "            # TODO 3: Pour chaque action a possible dans l'état s\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "\n",
    "                # TODO 4: Pour chaque transition possible (s' | s, a)\n",
    "                # Indice : utiliser env.unwrapped.P[s][action]\n",
    "                for transition_prob, next_state, reward, done in env.unwrapped.P[s][action]:\n",
    "\n",
    "                    # TODO 5: Calculer la contribution de cette transition\n",
    "                    # Formule : π(a|s) * p(s',r|s,a) * [r + γ * V(s')]\n",
    "                    v += action_prob * transition_prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            # TODO 6: Calculer delta = max(delta, |v_ancien - v_nouveau|)\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "\n",
    "            # TODO 7: Mettre à jour V(s)\n",
    "            V[s] = v\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        # TODO 8: Condition d'arrêt\n",
    "        if delta < theta:\n",
    "            print(f\"Convergence après {iteration} itérations\")\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39d90274-98a5-40ae-a5a1-c67d9b0f4527",
   "metadata": {
    "id": "39d90274-98a5-40ae-a5a1-c67d9b0f4527"
   },
   "source": [
    "# Test avec une politique aléatoire uniforme\n",
    "print(\"\\nTest : Évaluation d'une politique aléatoire\")\n",
    "random_policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "V_random = policy_evaluation(env, random_policy, discount_factor=0.9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser l'effet de gamma\n",
    "def visualize_value_functions_by_gamma_pe(env, policy, gamma_list):\n",
    "    fig, axes = plt.subplots(1, len(gamma_list), figsize=(5 * len(gamma_list), 6))\n",
    "\n",
    "    for idx, gamma in enumerate(gamma_list):\n",
    "        v_gamma = policy_evaluation(env, policy, discount_factor=gamma)\n",
    "        value_grid = v_gamma.reshape(8, 8)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(value_grid, cmap='YlOrRd')\n",
    "        ax.set_title(f'Value Function\\nγ = {gamma}', fontsize=12)\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                ax.text(j, i, f'{value_grid[i, j]:.3f}', ha='center', va='center',\n",
    "                        fontsize=8, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "_PMna9dbwO1Q"
   },
   "id": "_PMna9dbwO1Q",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "discount_factors = [0.8, 0.9, 0.95, 1.0]\n",
    "visualize_value_functions_by_gamma_pe(env, random_policy, discount_factors)"
   ],
   "metadata": {
    "id": "4oVJ9sTl3Qgf"
   },
   "id": "4oVJ9sTl3Qgf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a259668f-2c51-4d65-b82a-eb55c99d86b9",
   "metadata": {
    "id": "a259668f-2c51-4d65-b82a-eb55c99d86b9"
   },
   "source": [
    "## Partie 2 : Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "id": "332cbf62-964b-473b-a70b-3484377e1221",
   "metadata": {
    "id": "332cbf62-964b-473b-a70b-3484377e1221"
   },
   "source": [
    "def policy_improvement(env, V, discount_factor=0.9):\n",
    "    \"\"\"\n",
    "    Améliore une politique en la rendant greedy par rapport à V.\n",
    "\n",
    "    Rappel du cours :\n",
    "    π'(s) = argmax_a Σ_{s',r} p(s',r|s,a) * [r + γ * V(s')]\n",
    "\n",
    "    Args:\n",
    "        env: L'environnement Gym\n",
    "        V: Fonction de valeur\n",
    "        discount_factor: Facteur d'actualisation γ\n",
    "\n",
    "    Returns:\n",
    "        policy: Nouvelle politique améliorée (matrice |S| x |A|)\n",
    "    \"\"\"\n",
    "    # TODO 9: Initialiser une nouvelle politique (déterministe)\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # TODO 10: Pour chaque état\n",
    "    for s in range(env.observation_space.n):\n",
    "\n",
    "        # TODO 11: Calculer Q(s, a) pour chaque action\n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "\n",
    "        for a in range(env.action_space.n):\n",
    "\n",
    "            # TODO 12: Pour chaque transition (s' | s, a)\n",
    "            for transition_prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
    "\n",
    "                # TODO 13: Calculer Q(s,a) = Σ p(s'|s,a) * [r + γ * V(s')]\n",
    "                action_values[a] += transition_prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "        # TODO 14: Choisir la meilleure action (politique greedy)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[s][best_action] = 1.0\n",
    "\n",
    "    return policy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58ef9598-4493-496d-a23a-c16072df6be5",
   "metadata": {
    "id": "58ef9598-4493-496d-a23a-c16072df6be5"
   },
   "source": [
    "## Partie 3 : Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "06c28514-cc9c-495d-b961-a59982b2de87",
   "metadata": {
    "id": "06c28514-cc9c-495d-b961-a59982b2de87"
   },
   "source": [
    "def policy_iteration(env, discount_factor=0.9, max_iterations=200):\n",
    "    \"\"\"\n",
    "    Algorithme de Policy Iteration complet.\n",
    "\n",
    "    Principe :\n",
    "    1. Initialiser une politique aléatoire\n",
    "    2. Répéter :\n",
    "       a. Policy Evaluation : calculer V^π\n",
    "       b. Policy Improvement : obtenir π' greedy par rapport à V^π\n",
    "       c. Si π' = π, arrêter (politique optimale trouvée)\n",
    "\n",
    "    Args:\n",
    "        env: L'environnement Gym\n",
    "        discount_factor: Facteur d'actualisation γ\n",
    "        max_iterations: Nombre max d'itérations\n",
    "\n",
    "    Returns:\n",
    "        policy: Politique optimale\n",
    "        V: Fonction de valeur optimale\n",
    "    \"\"\"\n",
    "    # TODO 15: Initialiser une politique aléatoire\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # TODO 16: Évaluer la politique actuelle\n",
    "        V = policy_evaluation(env, policy, discount_factor)\n",
    "\n",
    "        # TODO 17: Améliorer la politique\n",
    "        new_policy = policy_improvement(env, V, discount_factor)\n",
    "\n",
    "        # TODO 18: Vérifier la convergence (si la politique n'a pas changé)\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            print(f\"Politique optimale trouvée après {i+1} itérations\")\n",
    "            return policy, V\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    print(f\"Nombre maximum d'itérations atteint ({max_iterations})\")\n",
    "    return policy, V"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5227817b-8745-4018-a20f-b5a2e39a906c",
   "metadata": {
    "id": "5227817b-8745-4018-a20f-b5a2e39a906c"
   },
   "source": [
    "# Test de Policy Iteration\n",
    "print(\"\\nExécution de Policy Iteration...\")\n",
    "optimal_policy_pi, V_pi = policy_iteration(env, discount_factor=0.9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser l'effet de gamma\n",
    "def visualize_value_functions_by_gamma_pi(env, policy, gamma_list):\n",
    "    fig, axes = plt.subplots(1, len(gamma_list), figsize=(5 * len(gamma_list), 6))\n",
    "\n",
    "    for idx, gamma in enumerate(gamma_list):\n",
    "        print(f\"γ = {gamma}\")\n",
    "        v_gamma = policy_evaluation(env, policy, discount_factor=gamma)\n",
    "        value_grid = v_gamma.reshape(8, 8)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(value_grid, cmap='YlOrRd')\n",
    "        ax.set_title(f'Value Function\\nγ = {gamma}', fontsize=12)\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                ax.text(j, i, f'{value_grid[i, j]:.3f}', ha='center', va='center',\n",
    "                        fontsize=8, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "8eHvS2_7wRkU"
   },
   "id": "8eHvS2_7wRkU",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "discount_factors = [0.8, 0.9, 0.95, 1.0]\n",
    "visualize_value_functions_by_gamma_pi(env, optimal_policy_pi, discount_factors)"
   ],
   "metadata": {
    "id": "xJKQ5XH_7VKe"
   },
   "id": "xJKQ5XH_7VKe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3d5d6c5-2af2-4d6d-b099-820f8456e1bc",
   "metadata": {
    "id": "d3d5d6c5-2af2-4d6d-b099-820f8456e1bc"
   },
   "source": [
    "## Partie 4 : Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc42cad4-46be-4faa-9f2a-7f7b93177c46",
   "metadata": {
    "id": "fc42cad4-46be-4faa-9f2a-7f7b93177c46"
   },
   "source": [
    "def value_iteration(env, discount_factor=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Algorithme de Value Iteration.\n",
    "\n",
    "    Principe : Mise à jour combinant évaluation et amélioration\n",
    "    V_{k+1}(s) = max_a Σ_{s',r} p(s',r|s,a) * [r + γ * V_k(s')]\n",
    "\n",
    "    Args:\n",
    "        env: L'environnement Gym\n",
    "        discount_factor: Facteur d'actualisation γ\n",
    "        theta: Seuil de convergence\n",
    "\n",
    "    Returns:\n",
    "        policy: Politique optimale extraite de V*\n",
    "        V: Fonction de valeur optimale V*\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(len(V)):\n",
    "            v = V[s]\n",
    "\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for transition_prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
    "                    action_values[a] += transition_prob * (reward + discount_factor * V[next_state])\n",
    "            V[s] = np.max(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        iteration += 1\n",
    "        if delta < theta:\n",
    "            print(f\"Convergence après {iteration} itérations\")\n",
    "            break\n",
    "    optimal_policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    return optimal_policy, V"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec28465c-2cfd-491b-aa7a-a51f7b10dde6",
   "metadata": {
    "id": "ec28465c-2cfd-491b-aa7a-a51f7b10dde6"
   },
   "source": [
    "# Test de Value Iteration\n",
    "print(\"\\nExécution de Value Iteration...\")\n",
    "optimal_policy_vi, V_vi = value_iteration(env, discount_factor=0.9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Experiment with different discount factors\n",
    "print(\"\\n=== Effect of Discount Factor ===\")\n",
    "discount_factors = [0.8, 0.9, 0.95, 1.0]\n",
    "# TO DO"
   ],
   "metadata": {
    "id": "AVe2o7xswBWW"
   },
   "id": "AVe2o7xswBWW",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser l'effet de gamma\n",
    "def visualize_value_functions_by_gamma_vi(env, gamma_list):\n",
    "    fig, axes = plt.subplots(1, len(gamma_list), figsize=(5 * len(gamma_list), 6))\n",
    "\n",
    "    for idx, gamma in enumerate(gamma_list):\n",
    "        _, v_gamma = value_iteration(env, discount_factor=gamma)\n",
    "        value_grid = v_gamma.reshape(8, 8)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(value_grid, cmap='YlOrRd')\n",
    "        ax.set_title(f'Value Function\\nγ = {gamma}', fontsize=12)\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                ax.text(j, i, f'{value_grid[i, j]:.3f}', ha='center', va='center',\n",
    "                        fontsize=8, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "yiGR9aqBwGZA"
   },
   "id": "yiGR9aqBwGZA",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "discount_factors = [0.8, 0.9, 0.95, 1.0]\n",
    "visualize_value_functions_by_gamma_vi(env, discount_factors)"
   ],
   "metadata": {
    "id": "79LVWTUS7n64"
   },
   "id": "79LVWTUS7n64",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ab4e772-d1c9-4618-bb93-fa8a789459df",
   "metadata": {
    "id": "2ab4e772-d1c9-4618-bb93-fa8a789459df"
   },
   "source": [
    "## Partie 5 : Visualisation et analyse"
   ]
  },
  {
   "cell_type": "code",
   "id": "2eaf92e6-ac2f-49d6-855f-08888ad58768",
   "metadata": {
    "id": "2eaf92e6-ac2f-49d6-855f-08888ad58768"
   },
   "source": [
    "def visualize_value_function(V, title=\"Fonction de Valeur\"):\n",
    "    \"\"\"Affiche la fonction de valeur sous forme de grille.\"\"\"\n",
    "    value_grid = V.reshape(8, 8)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(value_grid, cmap='YlOrRd')\n",
    "    plt.colorbar(label='V(s)')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            state = i * 8 + j\n",
    "            plt.text(j, i, f'{V[state]:.3f}',\n",
    "                    ha='center', va='center', fontsize=9, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_policy(policy, title=\"Politique\"):\n",
    "    \"\"\"Affiche la politique sous forme de flèches.\"\"\"\n",
    "    directions = ['←', '↓', '→', '↑']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    arrow_grid = np.full((8, 8), '', dtype='<U1')\n",
    "\n",
    "    for s in range(env.observation_space.n):\n",
    "        i, j = divmod(s, 8)\n",
    "        best_action = np.argmax(policy[s])\n",
    "        arrow_grid[i, j] = directions[best_action]\n",
    "\n",
    "    table = plt.table(cellText=arrow_grid, loc='center', cellLoc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "    table.scale(1, 1.5)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(16)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43ae5662-8996-4008-a76a-75ca5867d95f",
   "metadata": {
    "id": "43ae5662-8996-4008-a76a-75ca5867d95f"
   },
   "source": [
    "# TODO 28:\n",
    "## A changer selon ce qui doit être visualisé\n",
    "visualize_value_function(V_vi, \"Fonction de Valeur Optimale V*\")\n",
    "visualize_policy(optimal_policy_vi, \"Politique Optimale π*\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "835e604c-9cd7-4994-a158-5fbd02808b40",
   "metadata": {
    "id": "835e604c-9cd7-4994-a158-5fbd02808b40"
   },
   "source": [
    "## Partie 6 : Comparaison et analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfed07-f81f-4c0b-9eeb-25e89c45592d",
   "metadata": {
    "id": "e0dfed07-f81f-4c0b-9eeb-25e89c45592d"
   },
   "source": [
    "#### Question 1\n",
    "Comparez les résultats de Policy Iteration et Value Iteration.\n",
    "Les politiques obtenues sont-elles identiques ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c6482-9fb2-48d0-a66b-ccd81ed9bebd",
   "metadata": {
    "id": "ff0c6482-9fb2-48d0-a66b-ccd81ed9bebd"
   },
   "source": [
    "#### Question 2\n",
    "Comparez le nombre d'itérations nécessaires pour chaque algorithme.\n",
    "   Lequel converge le plus rapidement ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a98c6-9601-49ce-9688-407ee5f42899",
   "metadata": {
    "id": "aa2a98c6-9601-49ce-9688-407ee5f42899"
   },
   "source": [
    "#### Question 3\n",
    "Que se passe-t-il si on initialise policy_iteration avec la politique\n",
    "   optimale au lieu d'une politique aléatoire ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747cb3c-5926-4134-af3b-edd6b31d0ff9",
   "metadata": {
    "id": "0747cb3c-5926-4134-af3b-edd6b31d0ff9"
   },
   "source": [
    "#### Question 4\n",
    "Pourquoi la programmation dynamique nécessite-t-elle de connaître\n",
    "   complètement le modèle de l'environnement (P et R) ?\n",
    "   Quelles sont les limites de cette approche ?"
   ]
  },
  {
   "cell_type": "code",
   "id": "bba8339c-356c-4e08-bd6e-a267978ef508",
   "metadata": {
    "id": "bba8339c-356c-4e08-bd6e-a267978ef508"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
