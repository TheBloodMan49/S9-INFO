# YOLO

Ce TP n√©cessite une version r√©cente de Python 3, avec l'interpr√©teur Python et PIP dans le PATH.

## 0. Mise en place de l'environnement de travail

- Clonez ce d√©p√¥t:

    ```sh
    git clone https://github.com/GuillaumeMZ/sffs-yolo-tp.git
    ```

- Ouvrez un terminal dans le d√©p√¥t clon√©
- **macOS/Linux**: cr√©ez l'environnement virtuel et installez les d√©pendances:

    ```sh
    python3 -m venv venv
    source venv/bin/activate
    python3 -m pip install opencv-python ultralytics roboflow
    ```

- **Windows**: cr√©ez l'environnement virtuel et installez les d√©pendances:

    ```psh
    py -m venv venv
    .\venv\Scripts\activate
    py -m pip install opencv-python ultralytics roboflow
    ```

## 1. Cr√©ation et annotation d'un dataset pour le jeu pierre-feuille-ciseaux

### 1.1 Cr√©ation d'un *workspace*

Rendez-vous sur [Roboflow](https://roboflow.com) et cr√©ez-vous un compte. Vous serez redirig√© sur la page de cr√©ation d'un *workspace*. Entrez le nom que vous souhaitez, choisissez "Public Plan", cliquez sur "Continue", puis sur "Create Workspace" sur la page suivante.

### 1.2 Cr√©ation d'un projet

Sur la page principale de votre *workspace*, cliquez sur "Use my own data" (au centre, en bas). Choisissez "Build a custom model" sur la *pop-up*. Sur la page "Let's create your project", entrez un nom de projet au choix (si vous le voulez) et laissez les options par d√©faut. Cliquez sur "Create Public Project".

### 1.3 Ajout des classes

Cliquez sur l'onglet "Classes & Tags" dans la barre de gauche. Ajoutez les classes `rock`, `paper` et `scissors` en √©crivant "rock, paper, scissors" dans la barre de texte centrale puis cliquez sur le bouton "Add Classes".

### 1.4 Import et Annotation des donn√©es

#### 1.4.1 Import

Le dossier `images` contient 4 images (2 pierre, 1 feuille, 1 ciseaux) que vous devez t√©l√©verser dans votre projet Roboflow.

Dans votre projet Roboflow, cliquez sur "Select Files" au centre (onglet "Upload Data" dans la barre √† gauche), et s√©lectionnez les 4 images. Cliquez sur "Save and Continue".

#### 1.4.2 Annotation

Sur la page "Annotate", cliquez sur "Label Myself" √† droite de la page. L'interface d'annotation va alors s'ouvrir. S√©lectionnez l'outil "Bouding Box Tool" (2e icone en partant du haut sur la barre de droite).

Pour chaque image, dessinez un rectangle le plus serr√© possible autour de la main. Dans le menu qui s'ouvre √† gauche de l'image, s√©lectionnez la classe correspondante (`rock`, `paper` ou `scissors`) que vous avez cr√©√©e pr√©c√©demment.

Une fois les 4 images annot√©es, cliquez sur le bouton retour (fl√®che en haut √† gauche) pour revenir √† la vue d'ensemble, puis cliquez sur "Add 4 images to Dataset" (en haut √† droite). Sur la *popup* qui appara√Æt, choisissez la m√©thode "Split Images Between Train/Valid/Test". La r√©partition propos√©e par d√©faut devrait √™tre "Train: 3 images, Valid: 1 images, Test: 0 images". Si ce n'est pas le cas, d√©placez les curseurs pour y arriver. Cliquez sur "Add 4 Images".

### 1.5 G√©n√©ration du Dataset

Nous utiliserons le SDK Roboflow Python pour t√©l√©charger le dataset pr√™t √† l'entra√Ænement. Pour que le dataset soit utilisable par le code, il faut d'abord le "figer" dans une version.

Dans l'onglet "Dataset", cliquez sur le bouton "New Dataset Version" (en haut √† droite). Cliquez sur "Continue" √† chaque √©tape (laissez les valeurs par d√©faut) puis sur "Create".

Une fois le *dataset* cr√©√©, cliquez sur "Download Dataset" (en haut √† droite). Choisissez le format d'annotation "YOLOv12", s√©lectionnez "Show download code", puis cliquez sur "Continue". Copiez-collez le code dans le script `download_dataset.py` puis ex√©cutez le:

**MacOS/Linux:**

```sh
python3 download_dataset.py
```

**Windows:**

```sh
py download_dataset.py
```

Le dataset a √©t√© t√©l√©charg√© dans un sous-dossier. Utilisez l'explorateur de fichiers pour voir sa structure.

**Question: comment YOLO fait-il le lien entre une image et le fichier texte qui contient les informations la concernant ?**

√Ä partir de l'explorateur de fichiers, ouvrez l'un des fichiers .txt (soit dans train/labels ou dans valid/labels).

**Questions:**

- **√Ä quoi correspond une ligne ?**
- **Que signifie chaque √©l√©ment d'une ligne ?**
- **Pourquoi les nombres d√©cimaux sont-ils normalis√©s entre 0 et 1 ?**

### 1.6 Entra√Ænement

Nous allons maintenant entra√Æner notre (tr√®s) petit dataset avec YOLO. Lancez la commande suivante dans le terminal pour d√©marrer l'entra√Ænement (n'oubliez pas d'adapter le chemin de `data=`). Nous l'avons configur√© de la mani√®re suivante:

- Utilisation de YOLOv12-nano (t√©l√©charg√© automatiquement) comme base pour notre mod√®le
- 1 seule *epoch*
- Les images sont redimensionn√©es en 640x640 pixels.

```sh
yolo detect train data=chemin/vers/data.yaml epochs=1 imgsz=640
```

Le r√©sultat de l'entra√Ænement est contenu dans le dossier `runs/detect/train`.

- Les m√©triques r√©sultant de l'entra√Ænement (*precision*, *recall*, *F1*, etc.) se trouvent dans les fichiers .png contenus dans ce dossier.
- Notre mod√®le entra√Æn√© se trouve dans le sous-dossier `weights`. C'est lui que l'on chargerait avec YOLO pour effectuer une t√¢che de d√©tection pierre/feuille/ciseaux sur de nouvelles images.

Remarquez que le sous-dossier `weights` contient √† la fois `best.pt` et `last.pt`. **Question: pourquoi la derni√®re version du mod√®le n'est-elle pas n√©cessairement la meilleure ?**

Note: nous n'utiliserons pas ce mod√®le dans la suite de ce TP, pour plusieurs raisons:

- 4 images (dont 1 pour la validation) sont bien loin d'√™tre suffisantes pour obtenir un mod√®le performant
- 1 epoch est bien loin d'√™tre suffisante pour obtenir un mod√®le performant.

Ultralytics propose (entre autres) les recommendations suivantes:

- Au moins 1500 images par classe
- Au moins 10000 instances (objets labelis√©s) par classe
- Avoir entre 1 et 10% d'images ne contenant aucun objet
- D√©marrer avec 300 *epochs* et ajuster en fonction du r√©sultat (diminuer en cas d'*overfitting*, augmenter dans le cas contraire).

(Source: [Documentation Ultralytics](https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/))

## 2. Analyse des graphes de l'entra√Ænement d'un mod√®le

### 2.1 *Loss*

Ouvrez le fichier `dataset_part_2/results.png`. Nous nous concentrerons pour l'instant sur les graphes de perte/*loss* (les 6 √† gauche).

Rappel: la perte est une note de "nullit√©": plus elle est √©lev√©e, moins le mod√®le est bon. Plus elle est basse (le minimum √©tant 0), plus le mod√®le est bon. Elle est mesur√©e en comparant le r√©sultat attendu pour une tache par rapport au r√©sultat obtenu √† cette t√¢che.

Nous remarquons que les courbes concernant les donn√©es d'entra√Ænement (`train`) continuent de descendre au bout de 300 *epochs* alors que celles concernant les donn√©es de validation (`val`) ont stagn√© d√®s ~150 *epochs*.

**Question: √† quelle "maladie" cette constatation correspond-elle ?**

### 2.2 *Precision* et *recall*

Nous nous concentrons maintenant sur les graphes de *precision* et de *recall* (en haut √† droite).

**Questions:**

- **√Ä quoi correspond la *precision* ? √Ä quoi correspond le *recall* ?**
- **Dans nos r√©sultats, la *precision* et le *recall* sont proches de 1. Est-ce une bonne ou une mauvaise chose ? Pourquoi ?**
- **Pendant l'entra√Ænement, quel jeu de donn√©es est utilis√© pour calculer la *precision* et le *recall* ? `train`, `test`, ou `val` ?**

### 2.3 Matrice de confusion

Ouvrez maintenant le fichier `dataset_part_2/confusion_matrix_normalized.png`.

**Questions:**

- **Que repr√©sentent les cases fonc√©es de la diagonale haut-gauche vers bas-droite ? Pourquoi veut-on qu'elles soient le plus proche possible de 1.0 ?**
- **√Ä quoi correspondent la ligne et la colonne "background" ?**
- **Quelle est la classe que notre mod√®le reconna√Æt le mieux ? Le moins bien ?**
- **Quelles sont les deux classes que le mod√®le confond le plus ? Avec quelle autre classe ?**
- **Globalement, cette matrice est-elle satisfaisante ?**

### 2.4 Bonus: vitesse, pr√©cision et taille du mod√®le

Nous avons utilis√© le mod√®le YOLOv12-small comme base pour l'entra√Ænement. **Question: Si nous avions utilis√© YOLOv12x (extra large), quels auraient √©t√© les impacts en termes de vitesse d'ex√©cution, de pr√©cision et de taille du mod√®le sur le disque ? Quelle est la diff√©rence d'architecture entre un petit mod√®le et un grand mod√®le qui explique tout cela ?**

## 3. Utilisation d'un mod√®le pr√©-entra√Æn√© pour jouer √† pierre/feuille/ciseaux en live üî•

Le dossier `dataset_part_3` contient `model.pt`, un mod√®le bas√© sur YOLOv12-small, entra√Æn√© sur + de 3000 images pendant 300 *epochs* pour de la d√©tection de pierre/feuille/ciseaux. Nous l'utiliserons pour cr√©er un jeu pierre/feuille/ciseaux utilisant la webcam en live !

### R√®gles du jeu

L'√©cran est divis√© en deux zones:

- la zone de gauche correspond au joueur 1;
- la zone de droite correspond au joueur 2.

L'objectif est d'afficher, au centre de la fen√™tre, qui est le gagnant du pierre/feuille/ciseaux: le joueur 1 ou le joueur 2 (ou √©galit√©) ?

**Compl√©tez le TODO (ligne 36) du script `rock_paper_scissors_live.py` pour le faire fonctionner.**

Aide:

- `box` (ligne 35) est de type `Boxes` ([documentation](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Boxes)). Cette classe dispose notamment des propri√©t√©s `xyxy`, `conf` et `cls` qui sont des tenseurs PyTorch (et se manipulent comme des tableaux). **`Boxes` est une "structure de tableaux" et non pas un "tableau de structures", c'est-√†-dire qu'au lieu d'√©crire `box[0].abcd`, on √©crit `box.abcd[0]`.**
- `model` (ligne 4) est de type `Model` ([documentation](https://docs.ultralytics.com/reference/engine/model/#ultralytics.engine.model.Model)). Cette classe dispose d'une propri√©t√© `names` de type `dict[int, str]` qui permet d'acc√©der au nom d'une classe √† partir de son identifiant.
